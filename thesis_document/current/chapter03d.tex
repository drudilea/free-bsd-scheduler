\subsection{Tareas Extra}

Durante el proceso de desarrollo de los módulos, se fueron trabajando además algunas otras problemáticas encontradas.

\subsubsection{Modelado Específico de los Hilos \textit{Pinned}}

Durante el desarrollo de los módulos de encendido/apagado y monopolización de procesadores, surgió un problema crítico relacionado con la gestión de hilos \textit{pinned}, que ya había sido identificado en investigaciones anteriores.

En FreeBSD, los hilos \textit{pinned} deben ejecutarse exclusivamente en el procesador al que han sido asignados. En el contexto del planificador 4BSD modelado e implementado en la red de Petri de recursos, se modificó la lógica de selección de CPU. Originalmente, un hilo \textit{pinned} se reencolaba automáticamente en su procesador original sin condiciones adicionales. Sin embargo, con el desarrollo del modelo de la red de recursos, se introdujo una nueva condición: para que un hilo \textit{pinned} sea reencolado en su último procesador asignado, la transición que lo agrega a la cola del procesador debe estar habilitada.

Aunque esta modificación era necesaria para mantener el equilibrio en la carga de los núcleos del sistema, introdujo un problema adicional. Si, durante la ejecución de un hilo actual, se agrega otro hilo a la cola de ese procesador y simultáneamente se fija el hilo actual (es decir, se convierte en \textit{pinned}), la transición de reencolado del hilo \textit{pinned} podría quedar deshabilitada.

En tal situación, el planificador podría verse forzado a seleccionar un núcleo diferente o la cola global, lo que violaría la regla fundamental de los hilos \textit{pinned} del \textit{kernel} y podría provocar un \texttt{KERNEL PANIC}.

% \subsubsection{Solución del problema de afinidad (procesadores sobrecargados)}

% La primera de ellas, y la más crucial, fue un problema encontrado con la afinidad de los procesos.

% El síntoma ocurría durante nuestras pruebas de estrés; nos dimos cuenta de que los procesadores nunca alcanzaban el 100\% de su capacidad. Descubrimos que esto se debía a un problema de asignación de CPU que generaba una sobrecarga (overhead). El sistema operativo realizaba demasiados cambios de contexto entre los hilos, debido a un cambio en el código en relación con la afinidad, la limitación y la fijación a núcleos específicos.

% Estos cambios provocaban que el planificador eligiera los procesadores de forma aleatoria para encolar los hilos, sin considerar las indicaciones que podrían mejorar el rendimiento en la asignación de recursos y los cambios de contexto.

% % SKIPPED_TODO{IMAGEN DEL HTOP}

% A continuación, se dejan algunas pruebas de estrés realizadas previa y posteriormente a la implementación de la mejora. Si se observa detalladamente el programa htop en cada uno de los casos, se puede ver que en el primero, el promedio de uso de los procesadores es de un 78,62\% y el cálculo de todos los números primos hasta 100.000 tomó once segundos realizarlo (ocho veces, una por cada subproceso).

% En la segunda imagen, se muestran los resultados obtenidos al hacer los cambios correspondientes para mantener las funcionalidades de afinidad, limitación (bound) y la fijación a núcleos específicos (pin) en el planificador.

% Los cambios fueron positivos, logrando un uso pleno de los procesadores, que se mantuvieron al 100\% durante todas las pruebas de estrés realizadas y redujeron el tiempo de cálculo en un 63,63\%, completando la tarea en cuatro segundos.

% % SKIPPED_TODO{SEGUNDA IMAGEN DEL HTOP}


\subsubsection{Problema con la placa de red en el sistema operativo}\label{ch:problema_red}

Durante el desarrollo del proyecto integrador, identificamos un problema recurrente relacionado con el uso de la placa de red en nuestra máquina virtual. Este inconveniente surgía específicamente al utilizar SSH para facilitar la conexión con la máquina. Sin embargo, cuando se desactivaba la placa de red antes de iniciar la máquina virtual, el problema no ocurría, aunque esto también impedía el uso de SSH.\par

El problema técnico consistía en un \textit{kernel} panic provocado por un page fault durante el arranque del sistema operativo. Luego de cada incidente, el sistema generaba un archivo con información del estado previo al fallo, lo que nos permitió realizar un análisis más detallado.\par

Para abordar la situación, recurrimos al foro oficial de FreeBSD, donde presentamos nuestro caso. La discusión nos brindó diversas perspectivas y posibles soluciones, pero no logramos resolver el problema. Este intercambio de ideas se encuentra documentado en el hilo titulado Kernel panics when given a high workload, accesible en el foro oficial de FreeBSD\cite{bib6}.\par

Aunque no encontramos una solución definitiva, este proceso nos permitió adquirir conocimientos valiosos. Aprendimos a utilizar herramientas avanzadas para depurar el kernel, a interpretar archivos de crash dump generados durante los fallos y confirmamos que la comunidad de FreeBSD tiene un fuerte interés en iniciativas relacionadas con planificadores basados en Redes de Petri. Además, demostraron una disposición notable para ayudar en casos similares.\par

Consideramos que este es un punto relevante para futuras investigaciones y mejoras. Si bien no se trata de un problema bloqueante, su solución podría contribuir significativamente a mejorar la experiencia de desarrollo y operación del sistema.\par

Las capturas de pantalla y el análisis detallado del fallo se encuentran documentados en el Apéndice \ref{appendix:apC}.\par